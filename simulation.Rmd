---
title: "simulation"
author: "Stephen Lewandowski"
date: "November 1, 2018"
output: html_document
---
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

library(tidyverse)
library(rvest)
library(modelr)
library(mgcv)


set.seed(1)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```


```{r}
sim_regression = function(n, beta0 = 2, beta1 = 3) {
  
  sim_data = tibble(
    x = rnorm(n, mean = 1, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, 0, 1)
  )
  
  ls_fit = lm(y ~ x, data = sim_data)
  
  tibble(
    beta0_hat = coef(ls_fit)[1],
    beta1_hat = coef(ls_fit)[2]
  )
}
```

Run it a few times...

```{r}
sim_regression(n = 30, beta0 = 2, beta1 = 3)
```

Iterate simulations

```{r}
output = vector("list", length = 100)

for (i in 1:100) {
  output[[i]] = sim_regression(n = 30, beta0 = 2, beta1 = 3)
}

results =
  output %>% 
  bind_rows
```

```{r}
results %>% 
  summarize(mean_b0 = mean(beta0_hat),
            mean_b1 = mean(beta1_hat))

results %>% 
  ggplot(aes(x = beta0_hat, y = beta1_hat)) +
  geom_point()

```


A better way to iterate...rerun expresion in Purr

```{r}
rerun(100, sim_regression(30, 2, 3)) %>% 
bind_rows
```


## Increase Sample Size

```{r}
n_list = list("n_30"  = 30, 
              "n_60"  = 60, 
              "n_120" = 120, 
              "n_240" = 240)

output = vector("list", length = 4)

for (i in 1:4) {
  output[[i]] = rerun(100, sim_regression(n_list[[i]])) %>% 
    bind_rows
}
```



```{r}
simulate_n_regressions = function(n_runs = 100, n, beta0 = 2, beta1 = 3) {
  
  rerun(n_runs, sim_regression(n, beta0, beta1)) %>% 
    bind_rows()
  
}

simulate_n_regressions(100, 30, 20)


```

Use a list column

```{r}
sim_results = 
  tibble(sample_size = c(30, 60, 120, 240)) %>% 
  mutate(estimate_dfs = map(.x = sample_size, ~simulate_n_regressions(n = .x))) %>% 
  unnest

sim_results
```

```{r}
sim_results %>% 
  group_by(sample_size) %>% 
  summarize(emp_var_b1 = var(beta1_hat))
```

```{r}
sim_results %>% 
  ggplot(aes(x = beta0_hat, y = beta1_hat)) +
  geom_point() +
  facet_grid(~sample_size)
```


## Simulation: Publication Bias

New `sim_regression` function
```{r}
sim_regression = function(n_samp = 30, beta0 = 2, beta1 = 3) {
  
  sim_data = tibble(
    x = rnorm(n_samp),
    y = beta0 + beta1 * x + rnorm(n_samp, 0, sqrt(50))
  )
  
  ls_fit = lm(y ~ x, data = sim_data)
  
  broom::tidy(ls_fit) 
  
}

```


```{r}
sim_results = 
  tibble(beta1_true = 0:6) %>% 
  mutate(
    estimate_dfs = map(.x = beta1_true, ~simulate_n_regressions(n_runs = 10000, n = 30, beta1 = .x))
  )  
```

Simulate various slope values...

```{r}
sim_results = 
  sim_results %>% 
  unnest() %>% 
  filter(term == "x") %>% 
  select(beta1_true, estimate, p.value) %>% 
  mutate(significant = as.numeric(p.value < 0.05))
```



```{r}
sim_results %>% 
  ggplot(aes(x = estimate)) + geom_histogram() + 
  facet_grid(significant ~ beta1_true)
```


## Cross-Validation

```{r}
nonlin_df = tibble(
  id = 1:100,
  x = runif(100, 0, 1),
  y = 1 - 10 * (x - .3) ^ 2 + rnorm(100, 0, .3)
)

ggplot(nonlin_df, aes(x = x, y = y)) + geom_point() + theme_bw()
```


Partition into training and testing.

```{r}
train_df = sample_n(nonlin_df, 80)
test_df = anti_join(nonlin_df, train_df, by = "id")

ggplot(train_df, aes(x = x, y = y)) + 
  geom_point() + 
  geom_point(data = test_df, color = "red")
```

Fit a few models...
```{r}
lin_mod = lm(y ~ x, data = train_df)
nonlin_mod = mgcv::gam(y ~ s(x), data = train_df)
wiggly_mod = mgcv::gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df)
```

Show what these look like.

```{r}
train_df %>% 
  add_predictions(nonlin_mod) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + 
  geom_line(aes(y = pred), color = "red")


train_df %>% 
  add_predictions(wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + 
  geom_line(aes(y = pred), color = "red")


train_df %>% 
  add_predictions(lin_mod) %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + 
  geom_line(aes(y = pred), color = "red")

```

Make all plots together.

```{r}
train_df %>% 
  gather_predictions(lin_mod, nonlin_mod, wiggly_mod) %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_line(aes(y = pred), color = "red") + 
  facet_wrap(~model)
```

### RMSE

Use RMSE function.

```{r}
rmse(lin_mod, test_df)

rmse(nonlin_mod, test_df)

rmse(wiggly_mod, test_df)
```

### Iterate
Repeat cross-validations with modelr.

```{r}
cv_df = 
  crossv_mc(nonlin_df, 100) 
```


```{r}
cv_df %>% pull(train) %>% .[[1]] %>% as_tibble
```


Replace with ctual dataframes (less memory efficient)
```{r}
cv_df =
  cv_df %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))
```

```{r}
cv_df = 
  cv_df %>% 
  mutate(lin_mod    = map(train, ~lm(y ~ x, data = .x)),
         nonlin_mod = map(train, ~mgcv::gam(y ~ s(x), data = .x)),
         wiggly_mod = map(train, ~gam(y ~ s(x, k = 30), sp = 10e-6, data = .x))) %>% 
  mutate(rmse_lin    = map2_dbl(lin_mod, test, ~rmse(model = .x, data = .y)),
         rmse_nonlin = map2_dbl(nonlin_mod, test, ~rmse(model = .x, data = .y)),
         rmse_wiggly = map2_dbl(wiggly_mod, test, ~rmse(model = .x, data = .y)))
```



